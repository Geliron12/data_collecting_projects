# Web-crawler для обхода доменов
## crawling.py
Модуль **crawling.py** содержит класс **SiteCrawler**, в котором реализован функционал обхода переданных веб-доменов. Конструктор класса принимает на вход список names: list[str] - список заданных пользователем имен для сайтов, они не обязательно должны быть привязаны к url сайта, allowed_domains: list[list[str]] - список списков разрешенных доменов, start_urls: list[list[str]] - список списков стартовых url. Размерности исходных списков должны совпадать.\
Класс содержит в себе методы:\
    **make_spider** - создает веб-кроулер, принимает на вход имя кроулера, список доменов и список стартовых ссылок;\
    **crawling** - задействует переданный ей список кроулеров, с сохранением данных по путям из списка save_paths, также использует список имен names для инициализации параметров настройки обхода;\
    **crawl_all_sites** - метод, который совершает весь обход.\
Все полученные результаты по каждому кроулеру сохраняются в папки с названиями из списка names. Там лежат:\
    csv файл со всеми результатами;\
    json файл со статистикой;\
    pikle файлы с множествами уникальных адресов doc, pdf, docx файлов, не рабочих url и различных поддоменов.\
Для запуска из jupyter notebook нужно импортировать класс из модуля, создать экземпляр, вызвать функцию crawl_all_sites. **Важно!** - при повторном запуске придется перезагрузить kernel, ограничения процесса обхода из scrapy.