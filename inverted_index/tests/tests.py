from coding import NumberRepr, EliasCoding
from preprocessing import Tokenization
from indexing import InvertedIndex
from retrieval import IRetrieval
from utils import load_dataset
import pandas as pd

#############################–¢–µ—Å—Ç—ã –ø–æ —É–Ω–∞—Ä–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é —á–∏—Å–ª–∞###################################
def test_unary1():

    assert NumberRepr.Unary(1) == '1'

def test_unary2():

    assert NumberRepr.Unary(6) == '000001'

def test_unary3():

    assert NumberRepr.Unary(13) == '0000000000001'

def test_unary4():

    assert NumberRepr.Unary(958) == '0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001'

def test_unary5():

    assert NumberRepr.Unary(3) == '001'

#############################–¢–µ—Å—Ç—ã –ø–æ –±–∏–Ω–∞—Ä–Ω–æ–º—É –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é###################################
def test_binary1():
    assert NumberRepr.Binary(3) == bin(3)[2:]

def test_binary2():
    assert NumberRepr.Binary(100) == bin(100)[2:]

def test_binary3():
    assert NumberRepr.Binary(9272429834789) == bin(9272429834789)[2:]

def test_binary4():
    assert NumberRepr.Binary(0) == bin(0)[2:]

def test_binary5():
    assert NumberRepr.Binary(16) == bin(16)[2:]


#############################–¢–µ—Å—Ç—ã –ø–æ –±–∏–Ω–∞—Ä–Ω–æ–º—É –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é –±–µ–∑ —Å—Ç–∞—Ä—à–µ–≥–æ –±–∏—Ç–∞###################################
def test_binary_without_msb1():
    assert NumberRepr.binary_without_msb(3) == bin(3)[3:]

def test_test_binary_without_msb2():
    assert NumberRepr.binary_without_msb(100) == bin(100)[3:]

def test_test_binary_without_msb3():
    assert NumberRepr.binary_without_msb(9272429834789) == bin(9272429834789)[3:]

def test_test_binary_without_msb4():
    assert NumberRepr.binary_without_msb(0) == bin(0)[3:]

def test_test_binary_without_msb5():
    assert NumberRepr.binary_without_msb(16) == bin(16)[3:]

#############################–ì–∞–º–º–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≠–ª–∏–∞—Å–∞###################################
def test_gamma_encode1():
    assert EliasCoding.elias_gamma_encode(15) == '0001111'

def test_gamma_encode2():
    assert EliasCoding.elias_gamma_encode(100) == '0000001100100'

def test_gamma_encode3():
    assert EliasCoding.elias_gamma_encode(1251) == '000000000010011100011'

def test_gamma_encode4():
    assert EliasCoding.elias_gamma_encode(1) == '1'

def test_gamma_encode5():
    assert EliasCoding.elias_gamma_encode(839479875) == '00000000000000000000000000000110010000010010111001001000011'

#############################–ì–∞–º–º–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≠–ª–∏–∞—Å–∞###################################

def test_gamma_decode1():
    assert EliasCoding.elias_gamma_decode('0001111') == 15

def test_gamma_decode2():
    assert EliasCoding.elias_gamma_decode('0000001100100') == 100

def test_gamma_decode3():
    assert EliasCoding.elias_gamma_decode('000000000010011100011') == 1251

def test_gamma_decode4():
    assert EliasCoding.elias_gamma_decode('1') == 1

def test_gamma_decode5():
    assert EliasCoding.elias_gamma_decode('00000000000000000000000000000110010000010010111001001000011') == 839479875

#############################–î–µ–ª—å—Ç–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≠–ª–∏–∞—Å–∞###################################

def test_delta_encode1():
    assert EliasCoding.elias_delta_encode(14) == '00100110'

def test_delta_encode2():
    assert EliasCoding.elias_delta_encode(1) == '1'

def test_delta_encode3():
    assert EliasCoding.elias_delta_encode(100) == '00111100100'

def test_delta_encode4():
    assert EliasCoding.elias_delta_encode(1488) == '00010110111010000'

def test_delta_encode5():
    assert EliasCoding.elias_delta_encode(48954895485904389) == '000001110000101101111011000011011111010011011101000010011000000101'

#############################–î–µ–ª—å—Ç–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≠–ª–∏–∞—Å–∞###################################

def test_delta_decode1():
    assert EliasCoding.elias_delta_decode('00100110') == 14

def test_delta_decode2():
    assert EliasCoding.elias_delta_decode('1') == 1

def test_delta_decode3():
    assert EliasCoding.elias_delta_decode('00111100100') == 100

def test_delta_decode4():
    assert EliasCoding.elias_delta_decode('00010110111010000') == 1488

def test_delta_decode5():
    assert EliasCoding.elias_delta_decode('0000010011111000111111011011001000011110011101011') == 489548954859

#############################–¢–µ—Å—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞###################################

def test_tokenization1():
    assert Tokenization.tokenize_text('—è —Ç—ã –æ–Ω –æ–Ω–∞ –±—ã–ª') == []

def test_tokenization2():
    assert Tokenization.tokenize_text('') == []

def test_tokenization3():
    assert Tokenization.tokenize_text('–∞–±—ã—Ä–≤–∞–ª–≥') == ['–∞–±—ã—Ä–≤–∞–ª–≥']

def test_tokenization4():
    assert Tokenization.tokenize_text('–í–ø—Ä–æ—á–µ–º, —è –Ω–∏ —à–∏—à–∞ –Ω–µ —Å–º—ã—Å–ª—é –≤ –º–æ–µ–π –±–æ–ª–µ–∑–Ω–∏ –∏ –Ω–µ –∑–Ω–∞—é –Ω–∞–≤–µ—Ä–Ω–æ, —á—Ç–æ —É –º–µ–Ω—è –±–æ–ª–∏—Ç.') == ['—à–∏—à', '—Å–º—ã—Å–ª', '–º–æ', '–±–æ–ª–µ–∑–Ω', '–∑–Ω–∞', '–Ω–∞–≤–µ—Ä–Ω', '–±–æ–ª']

def test_tokenization5():
    assert Tokenization.tokenize_text('—é..–±—é–±—é–±..–±.—é–±—é. –¥–∞—É–∂') == ['–±—é–±—é–±', '—é–±', '–¥–∞—É–∂']

def test_tokenization6():
    assert Tokenization.tokenize_text('—é11–±—é–±—é–±1234–±—é–±—é456–¥–∞—É–∂') == ['—é11–±—é–±—é–±1234–±—é–±—é456–¥–∞—É–∂']

def test_tokenization7():
    assert Tokenization.tokenize_text('–Ø –¥—É—Ä–∞–∫ üôÇ') == ['–¥—É—Ä–∞–∫']  
#############################–¢–µ—Å—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏###################################

def test_indexation1():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    assert indexes['—à–∏—à'] == [4]

def test_indexation2():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    assert indexes['—á–µ–ª–æ–≤–µ–∫'] == [1, 2]

def test_indexation3():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    assert indexes['–Ω–∏–∫–∞–∫'] == [11]

def test_indexation4():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    assert indexes['—Å–º—ã—Å–ª'] == [4]

def test_indexation5():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    assert indexes.get('–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è',[]) == []

def test_indexation6():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    assert indexes.get('',[]) == []

#############################–¢–µ—Å—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞###################################
def test_retrieval1():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    relevant = IRetrieval.get_relevant_doc_indices('—á–µ–ª–æ–≤–µ–∫', indexes)
    assert relevant == {1, 2}

def test_retrieval2():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    relevant = IRetrieval.get_relevant_doc_indices('–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è', indexes)
    assert relevant == set()

def test_retrieval3():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    relevant = IRetrieval.get_relevant_doc_indices('–ø–µ—á–µ–Ω—å', indexes)
    assert relevant == {3}

def test_retrieval4():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    relevant = IRetrieval.get_relevant_doc_indices('–ø–æ–Ω–∏–º–∞—é', indexes)
    assert relevant == {9, 10}

def test_retrieval5():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    relevant = IRetrieval.get_relevant_doc_indices('—è', indexes)
    assert relevant == set()


def test_retrieval6():
    data = load_dataset('test_data.csv')
    data = Tokenization.tokenize_doc_corpus(data)
    indexes = InvertedIndex.create_inverted_index(data)
    relevant = IRetrieval.get_relevant_doc_indices('—á–µ–ª–æ–≤–µ–∫ –∑–ª–æ–π', indexes)
    assert relevant == {1, 2}